<a href="https://colab.research.google.com/github/Harithelamin/Computational_Intelligence/blob/main/AgeGenderDetection.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
# FYS-STK4155 - Project 3: Classifying age, and gender using Ridge, Lasso, Bagging, AdaBoost, and Convencual Neural Network


1. Import All library

import numpy as np, pandas as pd, matplotlib.pyplot as plt, random, io,  tensorflow as tf, tensorflow as keras

! pip install tensorflow
! pip install keras
#!pip  install tensorflow
#!pip  install tensorflow.keras

from tensorflow import keras
from keras import layers

2. Loading datasets age, and gender(FACE DATA) .CSV


database link: https://www.kaggle.com/datasets/nipunarora8/age-gender-and-ethnicity-face-data-csv?resource=download
#Get Data from Google Drive
from google.colab import files
uploaded = files.upload()
df = pd.read_csv(io.BytesIO(uploaded['age_gender.csv']))
df.head()
print(df)
#Get Data from local drive
import pandas as pd
from pathlib import Path
data = Path("C:/users/harit/Documents/age_gender.csv")
df = pd.read_csv(data.resolve(), sep=',')
df.head(3)

df.nunique()
#df.nunique()
#df.shape
df.nunique()

df.head(10)
df.info
df.describe()
I will foucus on age column
#Columns Headers
df.columns
# Add Network Network layers
model = keras.Sequential()
# Add an Embedding layer expecting input vocab of size 1000, and
# output embedding dimension of size 64.
model.add(layers.Embedding(input_dim=1000, output_dim=64))

# Add a LSTM layer with 128 internal units.
model.add(layers.LSTM(128))

# Add a Dense layer with 10 units.
model.add(layers.Dense(10))


model.summary()
#Add train model
from sklearn.model_selection import train_test_split
Col = ['gender','age']

y=df[Col]
X=df.drop(Col,axis=1)

X= pd.Series(X['pixels'])


X = X.apply(lambda x: x.split(' '))
X = X.apply(lambda x: np.array(list(map(lambda z: int(z), x))))
X = np.array(X)
X = np.stack(np.array(X), axis=0)
X = np.reshape(X, (-6, 48, 48))

X.shape
#Plot data
import matplotlib.pyplot as plt
plt.figure(figsize=(10,10))

for i,a in zip(np.random.randint(0, 20000, 58),range(1,26)):
    plt.subplot(5,5,a)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(X[i])
    plt.xlabel(
        "Age:"+str(y['age'].iloc[i])+       
        "  Gender:"+ str(y['gender'].iloc[i])
    )
plt.show()
y_age=np.array(y['age'])
y_gender=np.array(y['gender'])
#Split Gender X, and Y train, and test 
X_gender_train, X_gender_test, y_gender_train, y_gender_test = train_test_split(X, y_gender, train_size=0.9)
    
#Add train model
##Split Age X, and Y train, and test 
X_age_train, X_age_test, y_age_train, y_age_test = train_test_split(X, y_age, train_size=0.9)

#using Optimizer
#https://faroit.com/keras-docs/0.2.0/optimizers/
#Build Model
#Building Model using Adam Opptimizer

#Set Input Shape H, and L
Nomb_pix=2304
H=48
L=48

def RMSprop():
    return keras.optimizers.RMSprop(learning_rate = 0.001, rho = 0.9)


import tensorflow as tf
def build_model_with_Aadm_optimizer(num_classes, activation='adam', loss='sparse_categorical_crossentropy'):
    
    inputs = tf.keras.Input(shape=(H, L, 1))
    x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)
    x = tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu')(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu')(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu')(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(128, activation='relu')(x)
    outputs = tf.keras.layers.Dense(num_classes, activation=activation)(x)
    
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    #
    # add metrics
    metrics = [
        tf.keras.metrics.CategoricalAccuracy(name='accuracy'),
    ] 
    #Adam Optimizer
    optimizer = keras.optimizers.Adam(learning_rate=3e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-8)
    #model.compile(loss='categorical_crossentropy', metrics=['accuracy'] ,optimizer=optimizer)

    model.compile(optimizer='RMSprop', loss=loss, metrics = ['accuracy'])
    #model.compile(loss = 'mean_squared_error',     optimizer = 'sgd', metrics = [metrics.categorical_accuracy])

    #model.compile(loss = 'categorical_crossentropy', optimizer = RMSprop(), metrics = ['accuracy'])
    return model
#Age model History
age_model = build_model_with_Aadm_optimizer(1, activation='sigmoid', loss='binary_crossentropy')

gender_history= age_model.fit(
    X_age_train,
    y_age_train,
    validation_split=0.2,
    batch_size=64,
    epochs=20,
    callbacks=[tf.keras.callbacks.ReduceLROnPlateau()],
    verbose=0
)

#Age Learning carve
plt.title('Age Learning Curves')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.plot(age_history.history['loss'], label='train')
plt.plot(age_history.history['val_loss'], label='val')
plt.legend()
plt.show()


#Model Evaluate
#Age_history.evaluate(X_gender_test,y_gender_test)
#Evaluate Age Model
print("Evaluate Age Model....")
age_model.evaluate(X_gender_test, y_gender_test)
#Evaluate age Model
print("Evaluate age Model....")
gender_model.evaluate(X_gender_test, y_gender_test)


test_loss, test_acc = gender_model.evaluate(X_gender_test, y_gender_test)
print(test_acc)
CNN has achieved a test accuracy of more than 88%. It's very good
#Bagging
https://www.w3schools.com/python/python_ml_bagging.asp
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
X_gender_train.shape
X_gender_test.shape
# Import Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier 

np.random.seed(45)
clf = DecisionTreeClassifier(criterion="entropy")
print(df['age'].value_counts())
X = df[Col] # Input Variable 
y = df['age'] # Target Variable

X_train, X_test, y_train, y_test = train_test_split(X, y,train_size = 3065, random_state = 2)
clf.fit(X_train, y_train)
from sklearn.tree import DecisionTreeClassifier 
np.random.seed(45)
clf = DecisionTreeClassifier(criterion="entropy") 
clf = clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

from sklearn import metrics
print("Accuracy:", metrics.accuracy_score(y_test,y_pred))

from sklearn.ensemble import BaggingClassifier
model = BaggingClassifier(clf)
model.fit(X_train, y_train)
model.score(X_test,y_test)
from sklearn import tree
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(20, 10))
tree.plot_tree(clf, fontsize=10, filled=True, rounded=True)
plt.show()
Ridge, and lasso    
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge, RidgeCV, Lasso
from sklearn.preprocessing import StandardScaler
print("The dimension of X_train is {}".format(X_train.shape))
print("The dimension of X_test is {}".format(X_test.shape))
#Scale features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
#Model
lr = LinearRegression()

#Fit model
lr.fit(X_train, y_train)

#predict
prediction = lr.predict(X_test)

#actual
actual = y_test

train_score_lr = lr.score(X_train, y_train)
test_score_lr = lr.score(X_test, y_test)

print("The train score for lr model is {}".format(train_score_lr))
print("The test score for lr model is {}".format(test_score_lr))


#Ridge Regression Model
ridgeReg = Ridge(alpha=10)

ridgeReg.fit(X_train,y_train)

#train and test scorefor ridge regression
train_score_ridge = ridgeReg.score(X_train, y_train)
test_score_ridge = ridgeReg.score(X_test, y_test)

print("\nRidge Model............................................\n")
print("The train score for ridge model is {}".format(train_score_ridge))
print("The test score for ridge model is {}".format(test_score_ridge))
for a in alphas:
    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)
    ridge.fit(X, y)
    coefs.append(ridge.coef_)
#lasso
#Lasso regression model
print("\nLasso Model")
lasso = Lasso(alpha = 10)
lasso.fit(X_train,y_train)
train_score_ls =lasso.score(X_train,y_train)
test_score_ls =lasso.score(X_test,y_test)

print("The train score for lasso model is {}".format(train_score_ls))
print("The test score for lasso model is {}".format(test_score_ls))
#Adapost
# Create adaboost classifer object
from sklearn.ensemble import AdaBoostClassifier

# Import Vector Classifier
from sklearn.svm import SVC
#Import scikit-learn metrics 
from sklearn import metrics
svc=SVC(probability=True, kernel='linear')

# Create adaboost classifer object
object =AdaBoostClassifier(n_estimators=50, base_estimator=svc,learning_rate=1)

# Train Adaboost Classifer
model = object.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = model.predict(X_test)


# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
def mean_squared_error(y_true, y_predicted):
	
	# Calculating the loss or cost
	cost = np.sum((y_true-y_predicted)**2) / len(y_true)
	return cost
# Mean Squared Error

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error, r2_score


pipeline = make_pipeline(StandardScaler(), LinearRegression())
pipeline.fit(X_train, y_train)
# Calculate the predicted values

y_train_pred = pipeline.predict(X_train)
y_test_pred = pipeline.predict(X_test)


# Mean Squared Error
print('MSE train =',  (mean_squared_error(y_train, y_train_pred)))
print('MSE test = ',(mean_squared_error(y_test, y_test_pred)))
            
print('R2 Error for train data =',(r2_score(y_train, y_train_pred)))
print('R2 Error for test data =',(r2_score(y_test, y_test_pred)))


               
